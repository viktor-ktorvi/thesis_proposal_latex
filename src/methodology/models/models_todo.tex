\subsection*{Proposed models}
In the following section we will attempt to deduce which (more expressive) approaches and architectures
make sense to be applied to power grid datasets.
For this we will use the analysis presented in~\cite{expresive2022tutorial}.
The analysis presents four approaches to more expressive graph neural networks:
\begin{itemize}
    \item higher order
    \item feature augmentations
    \item substructure awareness
    \item bag of subgraphs
\end{itemize}

The analysis suggests that higher order methods work best for smaller graphs due to their
high computational complexity.
Seeing as though these methods are usually applied to molecular datasets and that the
sizes of those graphs are in the low tens of nodes~\cite{morris2020tudataset},
while power grids do come in that size, they also often come in the hundreds of nodes.
For that reasons we deem these methods unfeasible for achieving speedups in solving the OPF problem.
For the same reason we disregard the bag of subgraphs approach.
Additionally, since most power grids are trees and paths and their subgraphs are also trees and paths
it doesn't seem likely that keeping track of these sub-parts will lead to much benefit, so this
rules out substructure awareness as well.
What is most likely is that this task needs methods that have access to all the node features in the
grid so that a proper conservation of energy solution can be calculated.
All that's left is feature augmentation paired, likely, with graph transformers.
According to the analysis, these methods are applicable when large datasets are available,
which is the case since we can always generate more; when permutation invariance can be loosened,
which isn't a problem because we only need equivariance since we're doing a node level task; and
preprocessing is okay.

Specifically, for our case, preprocessing is okay as long as it's fast and as long as the
entire inference brings a large enough speedup over conventional solvers.
Even if the preprocessing of a single sample isn't very fast, there might be workarounds.
Since the grid operator needs to evaluate many hundreds or thousands of grids at once, the
evaluation could be done in parallel, and if the preprocessing could be shared over many
samples then the computational cost can be split evenly, making it negligible for a large
enough amount of samples.
For example, the evaluation dataset would consist of graphs with many topologies, but each
of those topologies would be represented by many samples.
If we were to have to calculate the laplacian eigenvectors or random walk encodings as part
of the preprocessing step, then that could be done for each topology once and then reused
for all the other samples.

Another source of models to try is the current
state-of-the-art\footnote{\url{https://paperswithcode.com/dataset/pascalvoc-sp}} on the
Long Range Graph Benchmark (LRGB)~\cite{dwivedi2022long}, seeing as though some of those
datasets, specifically the Peptides-struct(another regression task), look a lot like
medium voltage grids - like paths hundreds of nodes long.
The models that achieve good results on those benchmarks(usually graph transformers)
have potential to perform well on our task as well.

Based on this, the following lists the models I want to implement by the end of the thesis,
along with comments on why and how well I understand the approach:
\begin{itemize}
    \item Vanilla transformer~\cite{vaswani2017attention} - starting \("\)simple\("\) in terms of transformers, I want to
    see what the results will be using just stacked transformer encoders with no additional graph
    encodings, sort of in a deep sets way.
    I understand this approach relatively well.
    \item Transformer + graph encodings - moving on to thinking about different positional and structural encodings
    and appending them to the node embeddings before feeding them into the transformer encoders.
    I understand this approach well enough; it's the encodings that I'm not too familiar with and where I'll
    divert most of the mental work.
    \item GraphGPS~\cite{rampavsek2022recipe} - incorporating the previous approach with GNNs.
    I understand the approach well; it's, again, the encodings that I'll need to study more deeply.
\end{itemize}

These were the methods which I understood fairly well and that I've devoted time to understand.
The following are methods that I've come across that I find interesting, but don't understand at first glance
and haven't devoted the time into understanding.
I would like to dive deeper into them after implementing the previous ones.
\begin{itemize}
    \item Anything from this survey~\cite{muller2023attending}, as long as it's not too specialized for molecules.
    \item Virtual nodes~\cite{cai2023connection} - haven't read the paper, but it ranks highly on the LRGB.
    I've heard of virtual nodes but haven't seen them being used much, so I assumed that they'd just lead to oversmoothing
    quickly.
    I'd be very happy to see if they can be made to work.
    \item Other top performers on the LRGB.
\end{itemize}
