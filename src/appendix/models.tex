\subsection*{Models}

The models consist of a standard regression pipeline (with problem specific modifications) depicted in
Figure~\ref{fig:gnn_model_block_diagram}, where the input of the model, in this case the constants in the OPF problem,
are fed into a standard scaler \[z_{in} = \displaystyle\frac{x_{in} - \mu_{in}}{\sigma_{in}}\] then (optionally) through a GNN
feature extractor and then through a linear layer.
The outputs of the linear layer are then fed through and inverse standard scaler \[x_{out} = z_{out}\sigma_{out} + \mu_{out}\]
after which a binary mask is applied to leave only the variables of the OPF problem.
Those are, finally, combined with the constants to create an approximation of the optimal grid state $\approxpqva$.
All statistics are calculated strictly on the train set.

\mbox{}\\


One important trick that leads to better performance and that can only be applied to single-topology datasets is computing the statistics
$\mu_{in}, \sigma_{in}, \mu_{out}, \sigma_{out} \in \mathbb{R}^{d_{var}}$, where $d_{var}$ is the number of variables, for each variable
independently.
This is contrary to the more general case applicable to multi-topology datasets where we compute
$\mu_{in}, \sigma_{in}, \mu_{out}, \sigma_{out} \in \mathbb{R}^4$ only over the physical values themselves.
Additionally, in this case we calculate the input statistics after applying the node mask to the $\pqva$ matrix
(essentially deleting the target info) because that is what the input layer of the network will be receiving, but
use the untouched $\pqva$ matrix for calculating the output statistics because there is no reason for the zeros
to skew the statistics.

\mbox{}\\